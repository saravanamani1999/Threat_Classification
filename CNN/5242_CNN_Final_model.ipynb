{"cells":[{"cell_type":"markdown","source":["## References\n","\n","https://www.tensorflow.org/tutorials/images/classification (example code is tweaked)\n","\n","https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n","\n","https://www.tensorflow.org/addons/api_docs/python/tfa/image/equalize\n","\n","https://keras.io/examples/vision/grad_cam/\n","\n"],"metadata":{"id":"nMMUzg3vR67G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwICcpJRIcmF"},"outputs":[],"source":["# %tensorflow_version 2.x  \n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib \n","import matplotlib.pyplot as plt\n","import pandas as pd \n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras import layers\n","from keras.utils import image_dataset_from_directory\n","import matplotlib.cm as cm\n","import cv2\n","import tensorflow_addons as tfa\n","from tensorflow_addons.image import equalize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymqieu5C2eBs"},"outputs":[],"source":["train_data_dir = './frames/train/'\n","test_data_dir = './frames/test/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYzWVq4qkJ5W"},"outputs":[],"source":["batch_size = 32\n","img_height = 256\n","img_width = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Zg-yy3BkNHE","outputId":"9a33fdd5-5869-4d3f-d24d-8186d665cc59","executionInfo":{"status":"ok","timestamp":1679993701022,"user_tz":-480,"elapsed":87976,"user":{"displayName":"Mani","userId":"13549522688585089965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 21414 files belonging to 2 classes.\n","Using 17132 files for training.\n"]}],"source":["train_data = image_dataset_from_directory(\n","  train_data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  crop_to_aspect_ratio=True,\n","  color_mode ='grayscale',\n","  label_mode='binary',\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdULt9vKkqay","outputId":"44fcbcfc-98cc-4f38-dc96-d8fb581bd8d4","executionInfo":{"status":"ok","timestamp":1679993705692,"user_tz":-480,"elapsed":4687,"user":{"displayName":"Mani","userId":"13549522688585089965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 21414 files belonging to 2 classes.\n","Using 4282 files for validation.\n"]}],"source":["val_data = image_dataset_from_directory(\n","  train_data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  crop_to_aspect_ratio=True,\n","  color_mode ='grayscale',\n","  label_mode='binary',\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rz3YWaFkvcZ","outputId":"4d13c072-df6d-4185-b9ca-27cc0a191e1d","executionInfo":{"status":"ok","timestamp":1679993719066,"user_tz":-480,"elapsed":13378,"user":{"displayName":"Mani","userId":"13549522688585089965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4053 files belonging to 2 classes.\n"]}],"source":["test_data = image_dataset_from_directory(\n","  test_data_dir,\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  crop_to_aspect_ratio=True,\n","  color_mode ='grayscale',\n","  label_mode='binary',\n","  batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbJpCxsJlYr9","outputId":"5464f6ee-ca3e-405f-b205-e1a783d9e696","executionInfo":{"status":"ok","timestamp":1679993719066,"user_tz":-480,"elapsed":18,"user":{"displayName":"Mani","userId":"13549522688585089965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['norm', 'weap']\n"]}],"source":["class_names = train_data.class_names\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"CIz9CgiCY8J3"},"source":["Image pre-processing (Histogram Equalization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LcZ0PRT5liz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679993719067,"user_tz":-480,"elapsed":17,"user":{"displayName":"Mani","userId":"13549522688585089965"}},"outputId":"eda2e6c1-e6b7-4177-ed33-706b4672f2f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]}],"source":["#Histogram equalization\n","train_data = train_data.map(lambda x, y: (equalize(x), y))\n","val_data = val_data.map(lambda x, y: (equalize(x), y))\n","test_data = test_data.map(lambda x, y: (equalize(x), y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsiI1H2LRU3d"},"outputs":[],"source":["from tensorflow import data\n","AUTOTUNE = data.experimental.AUTOTUNE\n","train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n","val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"Y8_5eCTBY8J6"},"source":["Change made: \n","\n","Change MLP portion to 64x32x16x1 with sigmoid\n","\n","Data Augmentation as both loss and accuracy were decreasing -> need to mitigate overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQCWK-GevedM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679993721633,"user_tz":-480,"elapsed":2577,"user":{"displayName":"Mani","userId":"13549522688585089965"}},"outputId":"4fe80470-accd-4494-dde0-c80bab51124e"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]}],"source":["model = Sequential([\n","  tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n","  \n","  #Augmentation\n","  layers.RandomFlip(mode=\"horizontal\", input_shape=(img_height, img_width)),\n","  layers.RandomRotation(factor=(-0.125, 0.125)), #Rotate within -45 to 45 degs\n","  layers.RandomZoom(height_factor=-0.2, width_factor=None), #Zoom in by at most 20%\n","\n","  layers.Conv2D(64, 3, activation='relu'),\n","  layers.MaxPooling2D(3), #3x3 pooling\n","  tfa.layers.FilterResponseNormalization(),\n","\n","  layers.Conv2D(64, 3, 2, activation='relu'),\n","  layers.Dropout(0.2),\n","  tfa.layers.FilterResponseNormalization(),\n","\n","  layers.Conv2D(64, 3, 2, activation='relu'),\n","  layers.Dropout(0.2),\n","  tfa.layers.FilterResponseNormalization(),\n","\n","  layers.Conv2D(16, 3, 2, activation='relu'),\n","  layers.MaxPooling2D(2), #2x2 pooling\n","  layers.Dropout(0.2),\n","  tfa.layers.FilterResponseNormalization(),\n","\n","  \n","  layers.Flatten(),\n","\n","  layers.Dense(64, activation='relu'),\n","  layers.Dense(32, activation='relu'),\n","  layers.Dense(16, activation='relu'),\n","  \n","  layers.Dense(1, activation='sigmoid')\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXePk3uWvf43"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UrUov8zvi8M","outputId":"c6f5eea4-71be-46e6-85c2-c77d04fade6a","executionInfo":{"status":"ok","timestamp":1679993721636,"user_tz":-480,"elapsed":39,"user":{"displayName":"Mani","userId":"13549522688585089965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," rescaling (Rescaling)       (None, 256, 256, 1)       0         \n","                                                                 \n"," random_flip (RandomFlip)    (None, 256, 256, 1)       0         \n","                                                                 \n"," random_rotation (RandomRota  (None, 256, 256, 1)      0         \n"," tion)                                                           \n","                                                                 \n"," random_zoom (RandomZoom)    (None, 256, 256, 1)       0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 254, 254, 64)      640       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 84, 84, 64)       0         \n"," )                                                               \n","                                                                 \n"," filter_response_normalizati  (None, 84, 84, 64)       128       \n"," on (FilterResponseNormaliza                                     \n"," tion)                                                           \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 41, 41, 64)        36928     \n","                                                                 \n"," dropout (Dropout)           (None, 41, 41, 64)        0         \n","                                                                 \n"," filter_response_normalizati  (None, 41, 41, 64)       128       \n"," on_1 (FilterResponseNormali                                     \n"," zation)                                                         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 20, 20, 64)        36928     \n","                                                                 \n"," dropout_1 (Dropout)         (None, 20, 20, 64)        0         \n","                                                                 \n"," filter_response_normalizati  (None, 20, 20, 64)       128       \n"," on_2 (FilterResponseNormali                                     \n"," zation)                                                         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 9, 9, 16)          9232      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 4, 4, 16)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 4, 4, 16)          0         \n","                                                                 \n"," filter_response_normalizati  (None, 4, 4, 16)         32        \n"," on_3 (FilterResponseNormali                                     \n"," zation)                                                         \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 64)                16448     \n","                                                                 \n"," dense_1 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                528       \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 103,217\n","Trainable params: 103,217\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7aSjz4M1Y8J7"},"source":["To understand the model's strengths and weaknesses, we train and visualize the activation maps and kernels.\n","\n","Full train and validation sets, for 15 epochs:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qaw723hovlP2","outputId":"12ca936b-bf0d-4126-f9b9-b3f466381e8e"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  2/536 [..............................] - ETA: 22:56:58 - loss: 1.0488 - accuracy: 0.3750"]}],"source":["epochs=100\n","history = model.fit(\n","  train_data,\n","  validation_data=val_data,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96P-rACF_osj"},"outputs":[],"source":["model.evaluate(test_data)[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtyHtpkLvqyj"},"outputs":[],"source":["def plot_loss_acc_graphs(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    epochs_range = range(epochs)\n","\n","    plt.figure(figsize=(8, 8))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs_range, acc, label='Training Accuracy')\n","    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.title('Training and Validation Accuracy')\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs_range, loss, label='Training Loss')\n","    plt.plot(epochs_range, val_loss, label='Validation Loss')\n","    plt.legend(loc='upper right')\n","    plt.title('Training and Validation Loss')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saOcT3AoY8J9"},"outputs":[],"source":["plot_loss_acc_graphs(history)"]},{"cell_type":"markdown","metadata":{"id":"VVfULZ8ZY8J9"},"source":["The training and validation performance improves consistently for 15 epochs. The training performance is worse than validation due to dropout. However, the testing accuracy is poor. This could be because the model is underfitted as the loss has not saturated."]},{"cell_type":"markdown","metadata":{"id":"vPqICJl6uY3M"},"source":["### Exploring filters of CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwAMjY7gPiHZ"},"outputs":[],"source":["img_path = './frames/train/weap/1(LEE KANG WEI)_140.png'\n","img_size = (img_height, img_width)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOKgFrbleBPq"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqAvMcGFTwx9"},"outputs":[],"source":["i = 0\n","layer_indices = []\n","layer_names = []\n","for layer in model.layers:\n","\t# check for convolutional layer\n","  if 'conv' not in layer.name:\n","    i += 1\n","    continue\n","\t# summarize output shape\n","  print(i, layer.name, layer.output.shape)\n","  layer_indices.append(i)\n","  layer_names.append(layer.name)\n","  i += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Nm_jMZRY8J-"},"outputs":[],"source":["def get_filters(j, model):    \n","    layer = model.layers\n","    filters, biases = model.layers[j].get_weights()\n","    print(layer[j].name, filters.shape)\n","\n","    fig1=plt.figure(figsize=(8,8))\n","    columns = 8\n","    rows = 8\n","    n_filters = columns * rows\n","\n","    for i in range(1, n_filters+1):\n","        try:\n","            f = filters[:,:,:,i-1]\n","        except:\n","            continue\n","        fig1 = plt.subplot(rows, columns, i)\n","        fig1.set_xticks([])\n","        fig1.set_yticks([])\n","        plt.imshow(f[:,:,0], cmap='gray')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud6hZkn-6Kw3"},"outputs":[],"source":["for index in layer_indices:\n","    get_filters(index, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1QlUTPJUBwI"},"outputs":[],"source":["from keras.models import Model\n","\n","outputs = [model.layers[i].output for i in layer_indices]\n","model_filters = Model(inputs=model.inputs, outputs=outputs)\n","img = keras.preprocessing.image.load_img(img_path, target_size=img_size, grayscale=True)\n","img_array = keras.preprocessing.image.img_to_array(img)\n","img_array = np.expand_dims(img_array, axis=0)\n","img_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FTS1Vv1W_4l"},"outputs":[],"source":["def show_activation_maps(img_array):\n","  feature_maps = model_filters.predict(img_array)\n","  row = 8\n","  column = 8\n","  print(len(feature_maps[1]))\n","  for i in range(len(feature_maps)):\n","    fmap = feature_maps[i]\n","    if (i == 3):\n","      row = 4\n","      column = 4\n","      \n","    fig = plt.figure(figsize=(12,12))\n","    for i in range(1, row*column+1):\n","      fig = plt.subplot(row,column,i)\n","      fig.set_xticks([])\n","      fig.set_yticks([])\n","      plt.imshow(fmap[0,:,:,i-1], cmap='gray')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzBkjtNKY8J_"},"outputs":[],"source":["show_activation_maps(img_array)"]},{"cell_type":"markdown","metadata":{"id":"_4L9HSOjY8KA"},"source":["The activation maps detect both person and background. We see a more gradual extraction of general features for deeper layers\n","The normalization of filter response gives better contrasts overall."]},{"cell_type":"markdown","metadata":{"id":"koHwQnGWb69z"},"source":["### Grad CAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx9kHzCGb6f3"},"outputs":[],"source":["import matplotlib.cm as cm\n","from IPython.display import Image, display\n","\n","img_size = (img_height, img_width)\n","\n","last_conv_layer_name = layer_names[-1]\n","\n","display(Image(img_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6r_S-XZgq6V"},"outputs":[],"source":["img = keras.preprocessing.image.load_img(img_path, target_size=img_size, grayscale=True)\n","img_array = keras.preprocessing.image.img_to_array(img)\n","img_array = np.expand_dims(img_array, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oemlkQwDrgxV"},"outputs":[],"source":["def gradcam(img_array, model, last_conv_layer_name):\n","\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    with tf.GradientTape() as tape:\n","      last_conv_layer_output, preds = grad_model(img_array)\n","      pred_index = tf.argmax(preds[0])\n","      class_channel = preds[:, pred_index]\n","\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    \n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmMY196Hukcf"},"outputs":[],"source":["def show_grad(img_array, model, last_conv_layer_name, cam_path=\"./cam.jpg\"):\n","    heatmap = gradcam(img_array, model, last_conv_layer_name)\n","\n","    plt.matshow(heatmap)\n","    plt.show()\n","\n","    alpha = 0.4\n","\n","    cam_img = keras.preprocessing.image.load_img(img_path)\n","    cam_img = keras.preprocessing.image.img_to_array(cam_img)\n","\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    jet = cm.get_cmap(\"jet\")\n","\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((cam_img.shape[1], cam_img.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    superimposed_img = jet_heatmap * alpha + cam_img\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","    \n","    superimposed_img.save(cam_path)\n","\n","    display(Image(cam_path))\n","\n","    prediction = model.predict(img_array)\n","    print(\"Predicted class: \", prediction)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7rqZM-9Y8KB"},"outputs":[],"source":["show_grad(img_array, model, last_conv_layer_name)"]},{"cell_type":"markdown","metadata":{"id":"yjW7mEVjY8KC"},"source":["The heatmap shows the person is now greatly responsible for the classification. However, the background still makes some contributions.\n","This is much better than checkpoint 3. We believe that the model is underfitting. We expect increasing epochs will result in more localized responses in the heatmap.\n","\n","Based on the activation maps, it seems that the convolutional layers are performing well.\n","\n","The denser MLP and use of sigmoid activations have improved the performance"]},{"cell_type":"markdown","metadata":{"id":"tvPius-rY8KC"},"source":["The sample weapon image is predicted as \"norm\". "]},{"cell_type":"markdown","metadata":{"id":"8FEV9-J9Y8KC"},"source":["The goal is to detect the person and weapon. Hence we make the following changes:\n","\n","2. Increase epoch count to __"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1I5NgNNqAp1-Q-T0UTP1O0ovRYRTnvCAm","timestamp":1679993069073}],"machine_shape":"hm"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}